Microsoft Windows [Version 10.0.18363.1379]
(c) 2019 Microsoft Corporation. All rights reserved.

C:\WINDOWS\system32>cd..

C:\Windows>d:

D:\>cd D:\hadoop-2.8.0\hadoop-2.8.0\sbin

D:\hadoop-2.8.0\hadoop-2.8.0\sbin>start-all.cmd
This script is Deprecated. Instead use start-dfs.cmd and start-yarn.cmd
starting yarn daemons

D:\hadoop-2.8.0\hadoop-2.8.0\sbin>hadoop fs -mkdir /input_dir
mkdir: Cannot create directory /input_dir. Name node is in safe mode.

D:\hadoop-2.8.0\hadoop-2.8.0\sbin>hadoop fs -mkdir /input_wcdir

D:\hadoop-2.8.0\hadoop-2.8.0\sbin>hadoop fs -put D:/input_file.txt /input_dir

D:\hadoop-2.8.0\hadoop-2.8.0\sbin>hadoop fs -put D:/input_file.txt /input_wcdir

D:\hadoop-2.8.0\hadoop-2.8.0\sbin>hadoop fs -ls /input_wcdir/
Found 1 items
-rw-r--r--   1 NRika supergroup       1888 2021-03-01 02:46 /input_wcdir/input_file.txt

D:\hadoop-2.8.0\hadoop-2.8.0\sbin>hadoop dfs -cat /input_wcdir/input_file.txt
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.
23   23   27  43   24   25   26   26   26   26   25   26  25
26   27   28  28   28   30   31   31   31   30   30   30  29
31   32   32  32   33   34   35   36   36   34   34   34  34
39   38   39  39   39   41   42   43   40   39   38   38  40
38   39   39  39   39   41   41   41   28   40   39   39  45
23   23   27  43   24   25   26   26   26   26   25   26  25
26   27   28  28   28   30   31   31   31   30   30   30  29
31   32   32  32   33   34   35   36   36   34   34   34  34
39   38   39  39   39   41   42   43   40   39   38   38  40
38   39   39  39   39   41   41   41   28   40   39   39  45
23   23   27  43   24   25   26   26   26   26   25   26  25
26   27   28  28   28   30   31   31   31   30   30   30  29
31   32   32  32   33   34   35   36   36   34   34   34  34
39   38   39  39   39   41   42   43   40   39   38   38  40
38   39   39  39   39   41   41   41   28   40   39   39  45
23   23   27  43   24   25   26   26   26   26   25   26  25
26   27   28  28   28   30   31   31   31   30   30   30  29
31   32   32  32   33   34   35   36   36   34   34   34  34
39   38   39  39   39   41   42   43   40   39   38   38  40
38   39   39  39   39   41   41   41   28   40   39   39  45
23   23   27  43   24   25   26   26   26   26   25   26  25
26   27   28  28   28   30   31   31   31   30   30   30  29
31   32   32  32   33   34   35   36   36   34   34   34  34
39   38   39  39   39   41   42   43   40   39   38   38  40
38   39   39  39   39   41   41   41   28   40   39   39  45
23   23   27  43   24   25   26   26   26   26   25   26  25
26   27   28  28   28   30   31   31   31   30   30   30  29
31   32   32  32   33   34   35   36   36   34   34   34  34
39   38   39  39   39   41   42   43   40   39   38   38  40
38   39   39  39   39   41   41   41   28   40   39   39  45
D:\hadoop-2.8.0\hadoop-2.8.0\sbin>hadoop jar D:/MapReduceClient.jar wordcount /input_wcdir/output_wcdir
Usage: wordcount <in> [<in>...] <out>

D:\hadoop-2.8.0\hadoop-2.8.0\sbin>hadoop jar D:/MapReduceClient.jar wordcount /input_wcdir /output_wcdir
21/03/01 02:59:17 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
21/03/01 02:59:17 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
21/03/01 02:59:18 INFO input.FileInputFormat: Total input files to process : 1
21/03/01 02:59:18 INFO mapreduce.JobSubmitter: number of splits:1
21/03/01 02:59:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local859052974_0001
21/03/01 02:59:19 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
21/03/01 02:59:19 INFO mapreduce.Job: Running job: job_local859052974_0001
21/03/01 02:59:19 INFO mapred.LocalJobRunner: OutputCommitter set in config null
21/03/01 02:59:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
21/03/01 02:59:19 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/03/01 02:59:19 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
21/03/01 02:59:19 INFO mapred.LocalJobRunner: Waiting for map tasks
21/03/01 02:59:19 INFO mapred.LocalJobRunner: Starting task: attempt_local859052974_0001_m_000000_0
21/03/01 02:59:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
21/03/01 02:59:19 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/03/01 02:59:19 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
21/03/01 02:59:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6af4bcc7
21/03/01 02:59:19 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/input_wcdir/input_file.txt:0+1888
21/03/01 02:59:19 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
21/03/01 02:59:19 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
21/03/01 02:59:19 INFO mapred.MapTask: soft limit at 83886080
21/03/01 02:59:19 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
21/03/01 02:59:19 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
21/03/01 02:59:19 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
21/03/01 02:59:19 INFO mapred.LocalJobRunner:
21/03/01 02:59:19 INFO mapred.MapTask: Starting flush of map output
21/03/01 02:59:19 INFO mapred.MapTask: Spilling map output
21/03/01 02:59:19 INFO mapred.MapTask: bufstart = 0; bufend = 2730; bufvoid = 104857600
21/03/01 02:59:19 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26212840(104851360); length = 1557/6553600
21/03/01 02:59:19 INFO mapred.MapTask: Finished spill 0
21/03/01 02:59:19 INFO mapred.Task: Task:attempt_local859052974_0001_m_000000_0 is done. And is in the process of committing
21/03/01 02:59:19 INFO mapred.LocalJobRunner: map
21/03/01 02:59:19 INFO mapred.Task: Task 'attempt_local859052974_0001_m_000000_0' done.
21/03/01 02:59:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local859052974_0001_m_000000_0
21/03/01 02:59:19 INFO mapred.LocalJobRunner: map task executor complete.
21/03/01 02:59:19 INFO mapred.LocalJobRunner: Waiting for reduce tasks
21/03/01 02:59:19 INFO mapred.LocalJobRunner: Starting task: attempt_local859052974_0001_r_000000_0
21/03/01 02:59:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
21/03/01 02:59:19 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/03/01 02:59:19 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
21/03/01 02:59:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@24469f1a
21/03/01 02:59:19 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2f66e4c2
21/03/01 02:59:19 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10
21/03/01 02:59:19 INFO reduce.EventFetcher: attempt_local859052974_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
21/03/01 02:59:19 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local859052974_0001_m_000000_0 decomp: 191 len: 195 to MEMORY
21/03/01 02:59:19 INFO reduce.InMemoryMapOutput: Read 191 bytes from map-output for attempt_local859052974_0001_m_000000_0
21/03/01 02:59:19 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191
21/03/01 02:59:19 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
21/03/01 02:59:19 INFO mapred.LocalJobRunner: 1 / 1 copied.
21/03/01 02:59:19 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
21/03/01 02:59:19 INFO mapred.Merger: Merging 1 sorted segments
21/03/01 02:59:19 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 186 bytes
21/03/01 02:59:19 INFO reduce.MergeManagerImpl: Merged 1 segments, 191 bytes to disk to satisfy reduce memory limit
21/03/01 02:59:19 INFO reduce.MergeManagerImpl: Merging 1 files, 195 bytes from disk
21/03/01 02:59:19 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
21/03/01 02:59:19 INFO mapred.Merger: Merging 1 sorted segments
21/03/01 02:59:19 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 186 bytes
21/03/01 02:59:19 INFO mapred.LocalJobRunner: 1 / 1 copied.
21/03/01 02:59:19 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
21/03/01 02:59:19 INFO mapred.Task: Task:attempt_local859052974_0001_r_000000_0 is done. And is in the process of committing
21/03/01 02:59:19 INFO mapred.LocalJobRunner: 1 / 1 copied.
21/03/01 02:59:19 INFO mapred.Task: Task attempt_local859052974_0001_r_000000_0 is allowed to commit now
21/03/01 02:59:19 INFO output.FileOutputCommitter: Saved output of task 'attempt_local859052974_0001_r_000000_0' to hdfs://localhost:9000/output_wcdir/_temporary/0/task_local859052974_0001_r_000000
21/03/01 02:59:19 INFO mapred.LocalJobRunner: reduce > reduce
21/03/01 02:59:19 INFO mapred.Task: Task 'attempt_local859052974_0001_r_000000_0' done.
21/03/01 02:59:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local859052974_0001_r_000000_0
21/03/01 02:59:19 INFO mapred.LocalJobRunner: reduce task executor complete.
21/03/01 02:59:20 INFO mapreduce.Job: Job job_local859052974_0001 running in uber mode : false
21/03/01 02:59:20 INFO mapreduce.Job:  map 100% reduce 100%
21/03/01 02:59:20 INFO mapreduce.Job: Job job_local859052974_0001 completed successfully
21/03/01 02:59:20 INFO mapreduce.Job: Counters: 35
        File System Counters
                FILE: Number of bytes read=604640
                FILE: Number of bytes written=1252227
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=3776
                HDFS: Number of bytes written=120
                HDFS: Number of read operations=13
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=4
        Map-Reduce Framework
                Map input records=30
                Map output records=390
                Map output bytes=2730
                Map output materialized bytes=195
                Input split bytes=113
                Combine input records=390
                Combine output records=21
                Reduce input groups=21
                Reduce shuffle bytes=195
                Reduce input records=21
                Reduce output records=21
                Spilled Records=42
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=4
                Total committed heap usage (bytes)=463470592
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=1888
        File Output Format Counters
                Bytes Written=120

D:\hadoop-2.8.0\hadoop-2.8.0\sbin>hadoop dfs -cat /output_wcdir/*
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.
23      12
24      6
25      18
26      36
27      12
28      24
29      6
30      24
31      24
32      18
33      6
34      30
35      6
36      12
38      24
39      66
40      18
41      24
42      6
43      12
45      6

D:\hadoop-2.8.0\hadoop-2.8.0\sbin>
